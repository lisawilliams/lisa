---
layout: post
title:  "Computational Journalism Conference 2022"
date:   2022-06-09 05:51:39 -0400
categories: tech
---

# 2022 Computation + Journalism Conference <br>
*June 9-11, 2022<br>
Brown Institute for Media Innovation<br>
Columbia University*<br><br>

[Conference website](https://cj2022.brown.columbia.edu/)<br>
[Conference schedule](https://cj2022.brown.columbia.edu/schedule)<br><br>

You can see these notes on my [Github](https://github.com/lisawilliams/lisa/blob/gh-pages/_posts/2022-06-09-computational-journalism-conference-2022.markdown) or on [my site (where you can see writeups of other projects)](https://lisawilliams.github.io/lisa/tech/2022/06/09/computational-journalism-conference-2022.html). <br><br>

# Session: AI for Everyone

Speakers: Swapneel Mehta (New York University), Christopher Brennan (Overtone), Zhouhan Chen (New York University), Jessica Davis (Gannett Localizer), Matt Macvey (NYC Media Lab), Steve Dorsey (Gannett)

[Swapneel Mehta](https://twitter.com/swapneel_mehta) demonstrated a system that shows a dynamic (and predictive?) model of engagement with news websites. (I notice they use [Seven Days Vermont](https://sevendaysvt.com) as an example. The tool is called SimPPL. 

* [Jessica Davis]() talks about the Localizer platform (which I regret not having seen). Someone asked a "what is your stack" q; Jessica says mostly Python, Chris says mainly Google Cloud based; Swapneel says Python (and says more about a stack that I don't catch, sadly) but also Postgres. He mentions Crowdtangle which is interesting since SimPPL looks like something that could fit the niche that Crowdtangle is in (since CT seems to be in maintenance mode and it doesn't seem like there will be more investment). 

* There's some talk about having "product people" in newsrooms. This taps on one thing that is a fundamental issue: few newsrooms are big enough to have a technologist or product manager, and the media spends so little on technology in general that very few commercial entities (or investors) sees them as a market to develop specialty tools for.  Mainly, newsrooms use tools developed for other use cases. 

* Chris: "Going out and scraping everything is a very bad way to build a dataset." (Good point.)

* Jessica is talking about a tool that can automate the process of writing a story over a number of locations (think, automating the writing of a story on unemployment numbers, only across 100 major metros). She calls this "generating relevant stories at scale that can inform our communities." Also says that these newsrooms need to "wake up," because these tools are part of everyday life around us. 

"Optimizing our internet for clicks and likes is going to be a nightmare when you can use [AI] to generate a huge number of slightly different or tailored versions of the same content." (I am paraphrasing Chris Brennan here). His point is that we will have to have algorithms that surface quality, not just approximate those through clicks and likes. (I think.)


